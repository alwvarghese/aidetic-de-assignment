{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a760d01-75c8-4418-97cb-00ce6a5b72e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, count, countDistinct, avg, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "# Define the schema for the JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"key\", StringType(), True),\n",
    "    StructField(\"click_data\", StructType([\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"url\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"geo_data\", StructType([\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"user_agent_data\", StructType([\n",
    "        StructField(\"browser\", StringType(), True),\n",
    "        StructField(\"os\", StringType(), True),\n",
    "        StructField(\"device\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Read the stream from Kafka\n",
    "streaming_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"your_kafka_bootstrap_servers\")\n",
    "    .option(\"subscribe\", \"your_kafka_topic\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert the value column from binary to string\n",
    "streaming_df = streaming_df.selectExpr(\"CAST(value AS STRING) as json_value\")\n",
    "\n",
    "# Parse the JSON data\n",
    "parsed_df = streaming_df.select(from_json(col(\"json_value\"), schema).alias(\"data\"))\n",
    "\n",
    "# Define the batch processing function\n",
    "def process_batch(batch_df, batch_id):\n",
    "    # Flatten the nested fields\n",
    "    flattened_df = batch_df.select(\n",
    "        col(\"data.key\").alias(\"key\"),\n",
    "        col(\"data.click_data.user_id\").alias(\"user_id\"),\n",
    "        col(\"data.click_data.timestamp\").alias(\"timestamp\"),\n",
    "        col(\"data.click_data.url\").alias(\"url\"),\n",
    "        col(\"data.geo_data.country\").alias(\"country\"),\n",
    "        col(\"data.geo_data.city\").alias(\"city\"),\n",
    "        col(\"data.user_agent_data.browser\").alias(\"browser\"),\n",
    "        col(\"data.user_agent_data.os\").alias(\"os\"),\n",
    "        col(\"data.user_agent_data.device\").alias(\"device\")\n",
    "    )\n",
    "    \n",
    "    # Calculate time spent (assuming each click represents a session)\n",
    "    # Here, we assume that the session duration is a fixed value, e.g., 5 minutes (300 seconds)\n",
    "    flattened_df = flattened_df.withColumn(\"time_spent\", expr(\"300\"))  # Replace with actual logic if available\n",
    "    \n",
    "    # Perform the aggregations\n",
    "    aggregated_df = flattened_df.groupBy(\"url\", \"country\").agg(\n",
    "        count(\"*\").alias(\"number_of_clicks\"),\n",
    "        countDistinct(\"user_id\").alias(\"number_of_unique_users\"),\n",
    "        avg(\"time_spent\").alias(\"average_time_spent\")\n",
    "    )\n",
    "    \n",
    "    # JDBC connection properties\n",
    "    jdbc_url = \"jdbc:mysql://your_mysql_host:3306/your_database\"\n",
    "    connection_properties = {\n",
    "        \"user\": \"your_username\",\n",
    "        \"password\": \"your_password\",\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "    \n",
    "    # Write the aggregated data to MySQL\n",
    "    aggregated_df.write.jdbc(url=jdbc_url, table=\"aggregated_clickstream\", mode=\"append\", properties=connection_properties)\n",
    "\n",
    "# Use foreachBatch to apply the batch processing function\n",
    "query = (parsed_df.writeStream\n",
    "         .foreachBatch(process_batch)\n",
    "         .option(\"checkpointLocation\", \"/mnt/checkpoint/\")\n",
    "         .start())\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataCo_clickstream_ingestion",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
